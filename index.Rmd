---
title: "Practical Machine Learning - Course Project"
author: "Alicia Rodriguez"
date: "4/18/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=TRUE)
library(caret)
library(pROC)
library(parallel)
library(doParallel)
```

## Executive Summary

## Methodology

Let's load the data and make a first exploratory analysis.
```{r loading_data}
training_original<-read.csv(file="./pml-training.csv", header = TRUE, as.is = TRUE)
testing_quiz_original<-read.csv(file="./pml-testing.csv", header = TRUE, as.is = TRUE)
```

The data set is extensive (thus, not shown here): 19622 samples for the training set, and 20 for the quiz test set, each comprised of 160 variables. 
```{r explore_dataset, eval=FALSE}
str(training_original)
head(training_original,10)
```

It seems like the data has many columns with multiple NA values. Let's calculate the percentage of NA values per column:
```{r na_percentage}
training_clean<-training_original
training_clean[training_clean==""]<-NA
colSums(is.na(training_clean))*100/length(training_clean[,1])
```

Two cases can be observed: columns with 0% of NA values, and columns with almost 98% of NA values. Therefore, it seems like imputing data is not going to be a good idea here since there are way more NAs than real values, thus we proceed to delete the columns with NA values. Besides, some other variables of no interest will be deleted such us $I$, $user_name$,and those related timestamps and temporal windows. Lastly, the column $classe$ (the output to predict) will be factored.
```{r delete_na_columns}
valid_columns<-colSums(is.na(training_clean))==0
training_clean<-training_clean[,valid_columns]
training_clean<-training_clean[,-which(names(training_clean) %in% c("I","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window"))]
training_clean$classe<-factor(training_clean$classe)
```

Now there are 19622 samples, with 52 predictors each and the dependent variable to predict, $classe$, which is a factor variable now. Since the number of variables is so high, showing scatter plots is pretty difficult. Just for illustrative reasons, let's plot only some variables.
```{r plot_features}
featurePlot(x = training_clean[,1:4], 
             y = training_clean$classe, 
             plot = "pairs",
             ## Add a key at the top
             auto.key = list(columns = 3))
```

Clear groups can be spotted at each plot, but they seem very mixed, so it's difficult to extract any hypothesis on the model. It may be more obvious correlations with other variables not represented here, but since the nunber is so high, we will not represent the whole set.

Next, let's pre-process the data. First, let's look at any possible near-zero-variance variable in order to delete it, so it does not cause any problem when resampling during cross-validation.
```{r testing_nzv}
nzv <- nearZeroVar(training_clean[,-53], saveMetrics= TRUE)
nzv[nzv$nzv,]
```

There are no variables with near-zero-variance. Next, since the number of predictors is so high, let's try to reduce them by looking for high correlations among them, as well as linear combinations.
```{r find_correlations}
descrCor <-  cor(training_clean[,-53])
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
highlyCorDescr
training_filtered <- training_clean[,-highlyCorDescr]

comboInfo<-findLinearCombos(training_filtered[,-32])
comboInfo
```

Several variables have been deleted for being highly correlated, whereas we have found no linear combinations among the remianing variables. Since we would like to not only predict, but also understand which variables have more impact on the prediction, we are not going to apply any other transformation (box-cox, pca, ...).

To test the influence of centering and scaling the data in some of the prediction models, lets create a centered and scaled data set from the original one.
```{r center_scale}
processed_clean <- preProcess(training_clean[,-53], 
                            method = c("center", "scale"))
training_clean_proc <- training_clean
training_clean_proc[,1:52]<- predict(processed_clean,training_clean[,-53])
```

Once the data is pre-processed, let's start to test different models. First, since the quiz test data set has no $classe$ column (i.e., the predicted class) associated with each sample, we need to create a validation set from the training data we have. 
```{r split_training_validation}
set.seed(825)
trainCleanIndex <- createDataPartition(training_clean$classe, p = .8, 
                                  list = FALSE, 
                                  times = 1)
set.seed(825)
trainFilteredIndex <- createDataPartition(training_filtered$classe, p = .8, 
                                  list = FALSE, 
                                  times = 1)
set.seed(825)
trainCleanProcIndex <- createDataPartition(training_clean_proc$classe, p = .8, 
                                  list = FALSE, 
                                  times = 1)

cleanDataTrain <- training_clean[ trainCleanIndex,]
cleanDataVal  <- training_clean[-trainCleanIndex,]

filteredDataTrain <- training_filtered[ trainFilteredIndex,]
filteredDataVal <- training_filtered[ trainFilteredIndex,]

cleanProcDataTrain <- training_clean_proc[ trainCleanProcIndex,]
cleanProcDataVal <- training_clean_proc[ trainCleanProcIndex,]
```

Next, we are going to start building a model not easily interpretable, but which usually gets the highest accuracy: a random forest model. In order to do so, parallel processing will be used as well as cross-validation using k-fold, with 10 folds. Ideally, we would like to repeat the k-fold cross-validation several times, but due to computation time constraints, we leave the repetitions set to 1. We are going to apply this model to the three data sets we have generated: with all variables, with no correlated variables, and with centered and scaled variables.
```{r rf_training_cleanData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 1,
                           allowParallel = TRUE)
set.seed(825)
cleanData_time <- system.time(
  randomForest_cleanDataFit <- train(y=cleanDataTrain$classe, 
                                     x=cleanDataTrain[,-53], 
                                     method="rf",
                                     trControl = fitControl))
stopCluster(cluster)
registerDoSEQ()
```

```{r rf_training_filteredData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
filteredData_time <- system.time(
  randomForest_filteredDataFit <- train(y=filteredDataTrain$classe, 
                                        x=filteredDataTrain[,-32], 
                                        method="rf",
                                        trControl = fitControl))
stopCluster(cluster)
registerDoSEQ()
```

```{r rf_training_cleanProcData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
cleanProcData_time <- system.time(
  randomForest_cleanProcDataFit <- train(y=cleanProcDataTrain$classe, 
                                         x=cleanProcDataTrain[,-53], 
                                         method="rf",
                                         trControl = fitControl))
stopCluster(cluster)
registerDoSEQ()
```

```{r rf_models_comparison}
confusionMatrix.train(randomForest_cleanDataFit)
confusionMatrix.train(randomForest_filteredDataFit)
confusionMatrix.train(randomForest_cleanProcDataFit)
```

We can see that the accuracy in the training set is quite high, around 99.5% in the three cases. Besides, the time needed to train the model for each data set is as follows:
```{r time_comparison}
times<-rbind(allVariables=cleanData_time,noCorrVariables=filteredData_time)
times<-rbind(times,centeredScaledVariables=cleanProcData_time)
times
```

The main predictors in order of importance for each model are the ones represented below:
```{r rf_predictors_importance}
varImpPlot(randomForest_cleanDataFit$finalModel,type=2)
varImpPlot(randomForest_filteredDataFit$finalModel,type=2)
varImpPlot(randomForest_cleanProcDataFit$finalModel,type=2)
```

Since we have a multiclass classification problem, we cannot apply linear regression nor classical logistic regression. So let's try some other methods tan can be applied to this type of problems to see how they compare to the random forest case: regularized discriminant analysis (rda), gradient boosting models (gbm), simple decision trees (30 trees), classical neural networks, and support vector machines. For some cases (neural networks and support vector machines) we applied the models to the data with and without centering and scaling to show the importance of the pre-processing in certain cases.
```{r tree_training}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 1,
                           allowParallel = TRUE)

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
treeFitAll <-train(y=cleanDataTrain$classe, 
                   x=cleanDataTrain[,-53],
                   method="rpart", tuneLength = 30,
                   trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()
```

```{r gbm_training}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
gbmFitAll <-train(y=cleanDataTrain$classe, 
                  x=cleanDataTrain[,-53],
                  method="gbm",
                  trControl = fitControl,
                  verbose=FALSE)
stopCluster(cluster)
registerDoSEQ()
```

```{r rda_training}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
rdaFitAll <- train(y=cleanDataTrain$classe, 
                  x=cleanDataTrain[,-53], 
                  method = "rda", tuneLength = 4,
                  trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()
```

```{r nnet_training_cleanData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825) 
nnetFitAll <- train(y=cleanDataTrain$classe, 
                    x=cleanDataTrain[,-53],
                    method = "nnet", 
                    trControl = fitControl,
                    trace=FALSE)
stopCluster(cluster)
registerDoSEQ()
```

```{r nnet_training_cleanProcData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825) 
nnetFitAllProc <- train(y=cleanProcDataTrain$classe,
                        x=cleanProcDataTrain[,-53], 
                        method = "nnet",
                        trControl = fitControl,
                        trace=FALSE)
stopCluster(cluster)
registerDoSEQ()
```

```{r svm_training_cleanData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825) 
svmFitAll <- train(y=cleanDataTrain$classe, 
                       x=cleanDataTrain[,-53],
                       method = "svmRadial",
                       trControl=fitControl)
stopCluster(cluster)
registerDoSEQ()
```

```{r svm_training_cleanProcData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825) 
svmFitAllProc <- train(y=cleanProcDataTrain$classe,
                       x=cleanProcDataTrain[,-53], 
                       method = "svmRadial",
                       trControl=fitControl)
stopCluster(cluster)
registerDoSEQ()
```

```{r confusion_matrix_comparison}
confusionMatrix.train(treeFitAll)
confusionMatrix.train(gbmFitAll)
confusionMatrix.train(rdaFitAll)
confusionMatrix.train(nnetFitAll)
confusionMatrix.train(nnetFitAllProc)
confusionMatrix.train(svmFitAll)
confusionMatrix.train(svmFitAllProc)
```

Finally, let's compare the in error results of all the models in a more straight-forward manner:
```{r comparing_models}
resamps <- resamples(list(RF = randomForest_cleanDataFit,
                          RF_FILTERED = randomForest_filteredDataFit,
                          RF_PROC = randomForest_cleanProcDataFit,
                          GBM = gbmFitAll,
                          TREE = treeFitAll,
                          RDA = rdaFitAll,
                          NN = nnetFitAll,
                          NN_PROC = nnetFitAllProc,
                          SVM = svmFitAll,
                          SVM_PROC = svmFitAllProc))
summary(resamps)
trellis.par.set(caretTheme())
dotplot(resamps, metric = "Accuracy")
```

