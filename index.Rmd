---
title: "Practical Machine Learning - Course Project"
author: "Alicia Rodriguez"
date: "4/18/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=TRUE)
library(caret)
library(pROC)
library(parallel)
library(doParallel)
```

## Executive Summary

This report describes the building of a prediction model for the multiclass classification problem of deciding if a physical exercise is well performed. The predictors available are the signals coming from several sensors placed in different places of the body of the person performing the physical exercise. Several models are fit using cross-validation and different data preprocessing options, checking the in and out sample errors in each case. The model with the highest accuracy is further explored with respect to resulting confusion matrix and variable importance order.  

## Methodology

Let's load the data and make a first exploratory analysis.
```{r loading_data}
training_original<-read.csv(file="./pml-training.csv", header = TRUE, as.is = TRUE)
testing_quiz_original<-read.csv(file="./pml-testing.csv", header = TRUE, as.is = TRUE)
```

The data set is extensive (thus, not shown here): 19622 samples for the training set, and 20 for the quiz test set, each comprised of 160 variables. 
```{r explore_dataset, eval=FALSE}
str(training_original)
head(training_original,10)
```

It seems like the data has many columns with multiple NA values. Let's calculate the percentage of NA values per column:
```{r na_percentage}
training_clean<-training_original
training_clean[training_clean==""]<-NA
colSums(is.na(training_clean))*100/length(training_clean[,1])
```

Two cases can be observed: columns with 0% of NA values, and columns with almost 98% of NA values. Therefore, it seems like imputing data is not going to be a good idea here since there are way more NAs than real values, thus we proceed to delete the columns with NA values. Besides, some other variables of no interest will be deleted such us $I$, $user\_name$,and those related timestamps and temporal windows. Lastly, the column $classe$ (the output to predict) will be factored.
```{r delete_na_columns}
valid_columns<-colSums(is.na(training_clean))==0
training_clean<-training_clean[,valid_columns]
training_clean<-training_clean[,-which(names(training_clean) %in% c("I","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window"))]
training_clean$classe<-factor(training_clean$classe)
```

Now there are 19622 samples, with 52 predictors each and the dependent variable to predict, $classe$, which is a factor variable now. Since the number of variables is so high, showing scatter plots is pretty difficult. Just for illustrative reasons, let's plot only some variables.
```{r plot_features}
featurePlot(x = training_clean[,1:4], 
             y = training_clean$classe, 
             plot = "pairs",
             ## Add a key at the top
             auto.key = list(columns = 3))
```

Clear groups can be spotted at each plot, but they seem very mixed, so it's difficult to extract any hypothesis on the model. It may be more obvious correlations with other variables not represented here, but since the nunber is so high, we will not represent the whole set.

Next, let's pre-process the data. First, let's look at any possible near-zero-variance variable in order to delete it, so it does not cause any problem when resampling during cross-validation.
```{r testing_nzv}
nzv <- nearZeroVar(training_clean[,-53], saveMetrics= TRUE)
nzv[nzv$nzv,]
```

There are no variables with near-zero-variance. Next, since the number of predictors is so high, let's try to reduce them by looking for high correlations among them, as well as linear combinations.
```{r find_correlations}
descrCor <-  cor(training_clean[,-53])
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
highlyCorDescr
training_filtered <- training_clean[,-highlyCorDescr]

comboInfo<-findLinearCombos(training_filtered[,-32])
comboInfo
```

Several variables have been deleted for being highly correlated, whereas we have found no linear combinations among the remianing variables. Since we would like to not only predict, but also understand which variables have more impact on the prediction, we are not going to apply any other transformation (box-cox, pca, ...).

To test the influence of centering and scaling the data in some of the prediction models, lets create a centered and scaled data set from the original one.
```{r center_scale}
processed_clean <- preProcess(training_clean[,-53], 
                            method = c("center", "scale"))
training_clean_proc <- training_clean
training_clean_proc[,1:52]<- predict(processed_clean,training_clean[,-53])
```

Once the data is pre-processed, let's start to test different models. First, since the quiz test data set has no $classe$ column (i.e., the predicted class) associated with each sample, we need to create a validation set from the training data we have in order to calculate the out sample error. 
```{r split_training_validation}
set.seed(825)
trainCleanIndex <- createDataPartition(training_clean$classe, p = .8, 
                                  list = FALSE, 
                                  times = 1)
set.seed(825)
trainFilteredIndex <- createDataPartition(training_filtered$classe, p = .8, 
                                  list = FALSE, 
                                  times = 1)
set.seed(825)
trainCleanProcIndex <- createDataPartition(training_clean_proc$classe, p = .8, 
                                  list = FALSE, 
                                  times = 1)

cleanDataTrain <- training_clean[ trainCleanIndex,]
cleanDataVal  <- training_clean[-trainCleanIndex,]

filteredDataTrain <- training_filtered[ trainFilteredIndex,]
filteredDataVal <- training_filtered[ trainFilteredIndex,]

cleanProcDataTrain <- training_clean_proc[ trainCleanProcIndex,]
cleanProcDataVal <- training_clean_proc[ trainCleanProcIndex,]
```

Next, we are going to start building a model not easily interpretable, but which usually gets the highest accuracy: a random forest model. In order to do so, parallel processing will be used as well as cross-validation using k-fold, with 10 folds. Ideally, we would like to repeat the k-fold cross-validation several times, but due to computation time constraints, we leave the repetitions set to 1. We are going to apply this model to the three data sets we have generated: with all variables, with no correlated variables, and with centered and scaled variables.
```{r rf_training_cleanData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 1,
                           allowParallel = TRUE)
set.seed(825)
cleanData_time <- system.time(
  randomForest_cleanDataFit <- train(y=cleanDataTrain$classe, 
                                     x=cleanDataTrain[,-53], 
                                     method="rf",
                                     trControl = fitControl))
stopCluster(cluster)
registerDoSEQ()
```

```{r rf_training_filteredData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
filteredData_time <- system.time(
  randomForest_filteredDataFit <- train(y=filteredDataTrain$classe, 
                                        x=filteredDataTrain[,-32], 
                                        method="rf",
                                        trControl = fitControl))
stopCluster(cluster)
registerDoSEQ()
```

```{r rf_training_cleanProcData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
cleanProcData_time <- system.time(
  randomForest_cleanProcDataFit <- train(y=cleanProcDataTrain$classe, 
                                         x=cleanProcDataTrain[,-53], 
                                         method="rf",
                                         trControl = fitControl))
stopCluster(cluster)
registerDoSEQ()
```

The time needed to train the model for each data set is shown in the following table. As can be seen, the model with the no correlated variables takes half of the time to process, which can be very interesting in some occasions. This time, since the computer used has enough computing power so as to process the three variants in a reasonable time, we are going to choose the option with highest accuracy.
```{r time_comparison}
times<-rbind(allVariables=cleanData_time,noCorrVariables=filteredData_time)
times<-rbind(times,centeredScaledVariables=cleanProcData_time)
times
```

Let's compare the accuracy of the random forest with the three data sets with respect to other models. Since we have a multiclass classification problem, we cannot apply linear regression nor classical logistic regression. So let's try some other methods tan can be applied to this type of problems: regularized discriminant analysis (rda), gradient boosting models (gbm), simple decision trees (with 30 trees), classical neural networks (nn), and support vector machines (svm). For some cases (neural networks and support vector machines) we applied the models to the data with and without centering and scaling to show the importance of the pre-processing in certain cases.
```{r tree_training}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 1,
                           allowParallel = TRUE)

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
treeFitAll <-train(y=cleanDataTrain$classe, 
                   x=cleanDataTrain[,-53],
                   method="rpart", tuneLength = 30,
                   trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()
```

```{r gbm_training}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
gbmFitAll <-train(y=cleanDataTrain$classe, 
                  x=cleanDataTrain[,-53],
                  method="gbm",
                  trControl = fitControl,
                  verbose=FALSE)
stopCluster(cluster)
registerDoSEQ()
```

```{r rda_training}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825)
rdaFitAll <- train(y=cleanDataTrain$classe, 
                  x=cleanDataTrain[,-53], 
                  method = "rda", tuneLength = 4,
                  trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()
```

```{r nnet_training_cleanData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825) 
nnetFitAll <- train(y=cleanDataTrain$classe, 
                    x=cleanDataTrain[,-53],
                    method = "nnet", 
                    trControl = fitControl,
                    trace=FALSE)
stopCluster(cluster)
registerDoSEQ()
```

```{r nnet_training_cleanProcData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825) 
nnetFitAllProc <- train(y=cleanProcDataTrain$classe,
                        x=cleanProcDataTrain[,-53], 
                        method = "nnet",
                        trControl = fitControl,
                        trace=FALSE)
stopCluster(cluster)
registerDoSEQ()
```

```{r svm_training_cleanData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825) 
svmFitAll <- train(y=cleanDataTrain$classe, 
                       x=cleanDataTrain[,-53],
                       method = "svmRadial",
                       trControl=fitControl)
stopCluster(cluster)
registerDoSEQ()
```

```{r svm_training_cleanProcData}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(825) 
svmFitAllProc <- train(y=cleanProcDataTrain$classe,
                       x=cleanProcDataTrain[,-53], 
                       method = "svmRadial",
                       trControl=fitControl)
stopCluster(cluster)
registerDoSEQ()
```

Finally, let's compare the in sample error results of all the models in a more straight-forward manner. In the plot we can observe that random forest models perform better than the rest, followed closely by the gradient boosting model. We can also observe that whereas centering and scaling has not impact in random forest and support vector machine models, it does have a huge impact when fitting neural networks.
```{r comparing_models}
resamps <- resamples(list(RF = randomForest_cleanDataFit,
                          RF_FILTERED = randomForest_filteredDataFit,
                          RF_PROC = randomForest_cleanProcDataFit,
                          GBM = gbmFitAll,
                          TREE = treeFitAll,
                          RDA = rdaFitAll,
                          NN = nnetFitAll,
                          NN_PROC = nnetFitAllProc,
                          SVM = svmFitAll,
                          SVM_PROC = svmFitAllProc))
summary(resamps)
trellis.par.set(caretTheme())
dotplot(resamps, metric = "Accuracy")
```

And also, let's compare the out sample error of all the models. Again, the random forest is the best option, even achieving a higher accuracy than in the training data. 
```{r predict_all_models}
pred_rf_cleanData <- predict(randomForest_cleanDataFit, newdata=cleanDataVal[,-53])
pred_rf_filteredData <- predict(randomForest_filteredDataFit, newdata=filteredDataVal[,-32])
pred_rf_cleanProcData <- predict(randomForest_cleanProcDataFit, newdata=cleanProcDataVal[,-53])
pred_gbm_cleanData <- predict(gbmFitAll, newdata=cleanDataVal[,-53])
pred_tree_cleanData <- predict(treeFitAll, newdata=cleanDataVal[,-53])
pred_rda_cleanData <- predict(rdaFitAll, newdata=cleanDataVal[,-53])
pred_nnet_cleanData <- predict(nnetFitAll, newdata=cleanDataVal[,-53])
pred_nnet_cleanProcData <- predict(nnetFitAllProc, newdata=cleanProcDataVal[,-53])
pred_svm_cleanData <- predict(svmFitAll, newdata=cleanDataVal[,-53])
pred_svm_cleanProcData <- predict(svmFitAllProc, newdata=cleanProcDataVal[,-53])
```

```{r out_error_comparison}
out_error <- rbind(RF = postResample(pred=pred_rf_cleanData, obs=cleanDataVal$classe), 
                   RF_FILTERED = postResample(pred=pred_rf_filteredData, obs=filteredDataVal$classe))
out_error <- rbind(out_error, 
                   RF_PROC = postResample(pred=pred_rf_cleanProcData, obs=cleanProcDataVal$classe))
out_error <- rbind(out_error, 
                   GBM = postResample(pred=pred_gbm_cleanData, obs=cleanDataVal$classe))
out_error <- rbind(out_error, 
                   TREE = postResample(pred=pred_tree_cleanData, obs=cleanDataVal$classe))
out_error <- rbind(out_error, 
                   RDA = postResample(pred=pred_rda_cleanData, obs=cleanDataVal$classe))
out_error <- rbind(out_error, 
                   NN = postResample(pred=pred_nnet_cleanData, obs=cleanDataVal$classe))
out_error <- rbind(out_error, 
                   NN_PROC = postResample(pred=pred_nnet_cleanProcData, obs=cleanProcDataVal$classe))
out_error <- rbind(out_error, 
                   SVM = postResample(pred=pred_svm_cleanData, obs=cleanDataVal$classe))
out_error <- rbind(out_error, 
                   SVM_PROC = postResample(pred=pred_svm_cleanProcData, obs=cleanProcDataVal$classe))

out_error
```

## Final Model

Since the model with best accuracy (both in the training and validation data) is the random forest when using all the variables preprocessed (centered and scaled), let's take a look at some of their main features, this is, the main information about the model with respect to the cross-validation performed, confusion matrix for the training and validations sets, the evolution of the accuracy depending on the number of selected predictors, and the predictors with the highest importance.
```{r rf_final_model}
randomForest_cleanProcDataFit
confusionMatrix.train(randomForest_cleanProcDataFit)
confusionMatrix(data=pred_rf_cleanProcData, reference=cleanProcDataVal$classe)

trellis.par.set(caretTheme())
plot(randomForest_cleanProcDataFit)

varImpPlot(randomForest_cleanProcDataFit$finalModel,type=2)
```
